{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6944ad2c",
   "metadata": {},
   "source": [
    "Data importeren en inspecteren\n",
    "\n",
    "1. Data collection\n",
    "    Waar haal je de data op?\n",
    "    Hoe lees je hem (automatisch) in?\n",
    "2. Data exploration\n",
    "    Kwaliteit samples?\n",
    "    Welke categorien? verbinden met AQL (importeer pdf)\n",
    "preprocessing\n",
    "3. Feature extraction\n",
    "     welke combinatie van eigenscahppen zijn relevant?\n",
    "4. Data storage\n",
    "    waar sla je de bewerkte data op? dus map met data orginieel en data bewerkt\n",
    "    centrale plaats van het model voor build en run\n",
    "5. Data selection\n",
    "    genoeg samples per categorie?\n",
    "    welke weglaten?\n",
    "    onderverdelen in train en test\n",
    "    \n",
    "Vragen opdrachtgever data:\n",
    "1. Met blaadjes?\n",
    "\n",
    "tips:\n",
    "patronen metadata: opzoeken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29de364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection\n",
    "\n",
    "# 1. kies methode/library\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, RandomCrop\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotImgs(lImg, rImg):\n",
    "    plt.style.use('dark_background') \n",
    "    plt.rcParams[\"font.size\"] = 10\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 7)\n",
    "\n",
    "    fig, (axL, axR) = plt.subplots(ncols=2, constrained_layout=True)\n",
    "    \n",
    "    imL = axL.imshow(np.squeeze(lImg))\n",
    "    imR = axR.imshow(np.squeeze(rImg))\n",
    "\n",
    "\n",
    "# 3. benoem paths\n",
    "\n",
    "train_path = '../data/images/train/'\n",
    "test_path = '../data/images/test/'\n",
    "\n",
    "\n",
    "# load as image\n",
    "\n",
    "# train_imgs = torchvision.io.read_image(train_path).float()\n",
    "# test_imgs = torchvision.io.read_image(test_path).float()\n",
    "\n",
    "#print(train_imgs.shape)\n",
    "\n",
    "# load as tensor\n",
    "\n",
    "transform = ToTensor()\n",
    "\n",
    "# Tensors from images stored at dataset_path \n",
    "training_data = ImageFolder(train_path, transform=transform)\n",
    "trainSize =len(training_data)\n",
    "\n",
    "# training_data = ImageFolder(train_path, transform=transform, target_transform=Lambda(lambda y: torch.zeros(4, dtype=torch.float).scatter_(dim=0, torch.tensor(y), value=1))\n",
    "# test_data = ImageFolder(test_path, transform=transform)\n",
    "                            \n",
    "            \n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "175bd50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of images found: 388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.folder.ImageFolder"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Amount of images found: {trainSize}\")\n",
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6455c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxList = range(trainSize-1, trainSize)\n",
    "\n",
    "for sample_idx in idxList:\n",
    "    sample = training_data[sample_idx]\n",
    "    \n",
    "    # Access the image and label from the sample\n",
    "    image, label = sample\n",
    "    print(label)\n",
    "    \n",
    "#     # Print or examine the sample\n",
    "#     if sample_idx % 1000 == 0:\n",
    "#         print(\"Sample:\", sample_idx,\"Image shape:\", image.shape,\"Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9506368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_images(folder_path):\n",
    "    counter = 0\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "            extension = os.path.splitext(filename)[1]\n",
    "            new_name = f\"image_{counter}{extension}\"\n",
    "            current_path = os.path.join(folder_path, filename)\n",
    "            new_path = os.path.join(folder_path, new_name)\n",
    "            \n",
    "            # Delete existing names\n",
    "            os.remove(new_path) if os.path.exists(new_path) else None\n",
    "\n",
    "            # Handle file name conflicts\n",
    "            while os.path.exists(new_path):\n",
    "                counter += 1\n",
    "                new_name = f\"image_{counter}{extension}\"\n",
    "                new_path = os.path.join(folder_path, new_name)\n",
    "\n",
    "            os.rename(current_path, new_path)\n",
    "            counter += 1\n",
    "            \n",
    "\n",
    "# folder_path = \"../data/train/normal\"\n",
    "# rename_images(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c65886",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = rename_images(train_path+'normal_apple')\n",
    "botch = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c53eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def import_images(dataset_path):\n",
    "    image_list = []\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "            image_path = os.path.join(dataset_path, filename)\n",
    "            image = Image.open(image_path)\n",
    "            image_list.append(image)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "#dataset_path = \"/path/to/your/dataset\"\n",
    "#images = import_images(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb093914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of images found: 353\n"
     ]
    }
   ],
   "source": [
    "dataset_paths = [\"../data/train/normal\", \"../data/train/blotch\", \"../data/train/rot\", \"../data/train/scap\"]\n",
    "train_imgs = []\n",
    "\n",
    "for path in dataset_paths:\n",
    "    dataset_path = path\n",
    "    train_imgs += import_images(dataset_path)\n",
    "    \n",
    "print(f\"Amount of images found: {len(train_imgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dcbe6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of images found: 0\n"
     ]
    }
   ],
   "source": [
    "train_imgs_check = import_images(\"../data/train\")\n",
    "print(f\"Amount of images found: {len(train_imgs_check)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3b1875a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=208x243>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=274x184>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=100x100>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=262x192>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=274x184>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=203x248>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=100x100>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=100x100>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=274x184>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=236x214>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs[352]\n",
    "train_imgs[:10]\n",
    "\n",
    "# All different size: preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40518809",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train/'\n",
    "test_path = '../data/test/'\n",
    "\n",
    "\n",
    "# load as image\n",
    "\n",
    "# train_imgs = torchvision.io.read_image(train_path).float()\n",
    "# test_imgs = torchvision.io.read_image(test_path).float()\n",
    "\n",
    "#print(train_imgs.shape)\n",
    "\n",
    "# load as tensor\n",
    "\n",
    "transform = ToTensor()\n",
    "\n",
    "# Tensors from images stored at dataset_path \n",
    "training_data = ImageFolder(train_path, transform=transform)\n",
    "trainSize =len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2b17ebf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:7\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.class_map = {'River': [1,0], 'Highway': [0,1]}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# convert data to numeric values\n",
    "\n",
    "train_size = len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6886ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
